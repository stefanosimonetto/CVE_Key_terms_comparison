{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train CVE description with no subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "227/227 [==============================] - 1s 3ms/step loss: \n",
      "Epoch 1 - F1 Score: 0.5412\n",
      "Saved best model\n",
      "[0.5412277425555728]\n",
      "2036/2036 [==============================] - 11s 5ms/step - loss: 2.0307 - accuracy: 0.4674 - val_loss: 1.5372 - val_accuracy: 0.5747\n",
      "Epoch 2/40\n",
      "227/227 [==============================] - 1s 3ms/step loss: 1\n",
      "Epoch 2 - F1 Score: 0.5964\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246]\n",
      "2036/2036 [==============================] - 11s 6ms/step - loss: 1.4063 - accuracy: 0.6073 - val_loss: 1.3792 - val_accuracy: 0.6161\n",
      "Epoch 3/40\n",
      "227/227 [==============================] - 1s 3ms/step loss:\n",
      "Epoch 3 - F1 Score: 0.6091\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966]\n",
      "2036/2036 [==============================] - 10s 5ms/step - loss: 1.2803 - accuracy: 0.6401 - val_loss: 1.3366 - val_accuracy: 0.6252\n",
      "Epoch 4/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 1.2108 - accura\n",
      "Epoch 4 - F1 Score: 0.6295\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647]\n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 1.2106 - accuracy: 0.6584 - val_loss: 1.2816 - val_accuracy: 0.6462\n",
      "Epoch 5/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 1.1592 \n",
      "Epoch 5 - F1 Score: 0.6295\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693]\n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 1.1592 - accuracy: 0.6717 - val_loss: 1.2679 - val_accuracy: 0.6448\n",
      "Epoch 6/40\n",
      "227/227 [==============================] - 1s 3ms/step los\n",
      "Epoch 6 - F1 Score: 0.6367\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954]\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 1.1212 - accuracy: 0.6801 - val_loss: 1.2475 - val_accuracy: 0.6499\n",
      "Epoch 7/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 1.0884 - accu\n",
      "Epoch 7 - F1 Score: 0.6445\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678]\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 1.0878 - accuracy: 0.6895 - val_loss: 1.2359 - val_accuracy: 0.6550\n",
      "Epoch 8/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 1.0\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 1.0572 - accuracy: 0.6974 - val_loss: 1.2209 - val_accuracy: 0.6569\n",
      "Epoch 9/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 1.0315 - accura\n",
      "Epoch 9 - F1 Score: 0.6467\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678, 0.6443462729359976, 0.6466693586797999]\n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 1.0313 - accuracy: 0.7047 - val_loss: 1.2200 - val_accuracy: 0.6575\n",
      "Epoch 10/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 1.007\n",
      "Epoch 10 - F1 Score: 0.6498\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678, 0.6443462729359976, 0.6466693586797999, 0.6498144580991387]\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 1.0073 - accuracy: 0.7090 - val_loss: 1.2186 - val_accuracy: 0.6571\n",
      "Epoch 11/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.9854 - ac\n",
      "Epoch 11 - F1 Score: 0.6515\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678, 0.6443462729359976, 0.6466693586797999, 0.6498144580991387, 0.6514860955998075]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9855 - accuracy: 0.7148 - val_loss: 1.2085 - val_accuracy: 0.6612\n",
      "Epoch 12/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.9\n",
      "Epoch 12 - F1 Score: 0.6538\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678, 0.6443462729359976, 0.6466693586797999, 0.6498144580991387, 0.6514860955998075, 0.6538206519091913]\n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 0.9630 - accuracy: 0.7225 - val_loss: 1.2043 - val_accuracy: 0.6611\n",
      "Epoch 13/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.9421 - accura\n",
      "Epoch 13 - F1 Score: 0.6551\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678, 0.6443462729359976, 0.6466693586797999, 0.6498144580991387, 0.6514860955998075, 0.6538206519091913, 0.6550915798286339]\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.9424 - accuracy: 0.7273 - val_loss: 1.2282 - val_accuracy: 0.6579\n",
      "Epoch 14/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.9228 \n",
      "Epoch 14 - F1 Score: 0.6599\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678, 0.6443462729359976, 0.6466693586797999, 0.6498144580991387, 0.6514860955998075, 0.6538206519091913, 0.6550915798286339, 0.6599244750823375]\n",
      "2036/2036 [==============================] - 9s 5ms/step - loss: 0.9226 - accuracy: 0.7310 - val_loss: 1.2068 - val_accuracy: 0.6628\n",
      "Epoch 15/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.9\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.9042 - accuracy: 0.7357 - val_loss: 1.2207 - val_accuracy: 0.6614\n",
      "Epoch 16/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.8881 - \n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.8877 - accuracy: 0.7398 - val_loss: 1.2092 - val_accuracy: 0.6666\n",
      "Epoch 17/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.8710 \n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.8705 - accuracy: 0.7452 - val_loss: 1.2340 - val_accuracy: 0.6614\n",
      "Epoch 18/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.8\n",
      "2036/2036 [==============================] - 9s 5ms/step - loss: 0.8536 - accuracy: 0.7490 - val_loss: 1.2332 - val_accuracy: 0.6639\n",
      "Epoch 19/40\n",
      "227/227 [==============================] - 1s 3ms/step loss:\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.8372 - accuracy: 0.7532 - val_loss: 1.2246 - val_accuracy: 0.6670\n",
      "Epoch 20/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.8216 - accu\n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 0.8218 - accuracy: 0.7566 - val_loss: 1.2277 - val_accuracy: 0.6666\n",
      "Epoch 21/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.8065 - accura\n",
      "2036/2036 [==============================] - 5s 3ms/step - loss: 0.8073 - accuracy: 0.7603 - val_loss: 1.2693 - val_accuracy: 0.6565\n",
      "Epoch 22/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.7921 - accu\n",
      "Epoch 22 - F1 Score: 0.6640\n",
      "Saved best model\n",
      "[0.5412277425555728, 0.5963509858941246, 0.6090936473846966, 0.6294804053444647, 0.6294871618718693, 0.6366572576675954, 0.6445002113763678, 0.6443462729359976, 0.6466693586797999, 0.6498144580991387, 0.6514860955998075, 0.6538206519091913, 0.6550915798286339, 0.6599244750823375, 0.6543995111529235, 0.6571028232027041, 0.6536938323828464, 0.6557762210628706, 0.6594730475570717, 0.6580294982762398, 0.6530720134350195, 0.6639983466971486]\n",
      "2036/2036 [==============================] - 5s 3ms/step - loss: 0.7918 - accuracy: 0.7657 - val_loss: 1.2306 - val_accuracy: 0.6710\n",
      "Epoch 23/40\n",
      "227/227 [==============================] - 1s 3ms/step loss: 0\n",
      "2036/2036 [==============================] - 6s 3ms/step - loss: 0.7778 - accuracy: 0.7684 - val_loss: 1.2600 - val_accuracy: 0.6677\n",
      "Epoch 24/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7620 - \n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 0.7627 - accuracy: 0.7731 - val_loss: 1.2553 - val_accuracy: 0.6640\n",
      "Epoch 25/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.7488 - accuracy: 0.7775 - val_loss: 1.2804 - val_accuracy: 0.6619\n",
      "Epoch 26/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.7373 - accuracy: 0.7790 - val_loss: 1.2710 - val_accuracy: 0.6652\n",
      "Epoch 27/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.7\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.7247 - accuracy: 0.7828 - val_loss: 1.3174 - val_accuracy: 0.6618\n",
      "Epoch 28/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.7106 - accura\n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 0.7110 - accuracy: 0.7858 - val_loss: 1.3040 - val_accuracy: 0.6657\n",
      "Epoch 29/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.6972 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6982 - accuracy: 0.7905 - val_loss: 1.3014 - val_accuracy: 0.6661\n",
      "Epoch 30/40\n",
      "227/227 [==============================] - 0s 971us/steposs: 0.6881 - accura\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6878 - accuracy: 0.7932 - val_loss: 1.3233 - val_accuracy: 0.6650\n",
      "Epoch 31/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.674\n",
      "2036/2036 [==============================] - 10s 5ms/step - loss: 0.6748 - accuracy: 0.7965 - val_loss: 1.3513 - val_accuracy: 0.6632\n",
      "Epoch 32/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6616 \n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.6619 - accuracy: 0.8009 - val_loss: 1.3681 - val_accuracy: 0.6630\n",
      "Epoch 33/40\n",
      "227/227 [==============================] - 1s 3ms/step loss:\n",
      "2036/2036 [==============================] - 9s 5ms/step - loss: 0.6525 - accuracy: 0.8028 - val_loss: 1.3579 - val_accuracy: 0.6628\n",
      "Epoch 34/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.6397 - accuracy: 0.8055 - val_loss: 1.3835 - val_accuracy: 0.6628\n",
      "Epoch 35/40\n",
      "227/227 [==============================] - 1s 3ms/step l\n",
      "2036/2036 [==============================] - 10s 5ms/step - loss: 0.6276 - accuracy: 0.8091 - val_loss: 1.4098 - val_accuracy: 0.6596\n",
      "Epoch 36/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6187 \n",
      "2036/2036 [==============================] - 6s 3ms/step - loss: 0.6184 - accuracy: 0.8120 - val_loss: 1.4032 - val_accuracy: 0.6600\n",
      "Epoch 37/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.6088 - accura\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.6088 - accuracy: 0.8151 - val_loss: 1.4299 - val_accuracy: 0.6601\n",
      "Epoch 38/40\n",
      "227/227 [==============================] - 1s 3ms/step loss: 0\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.5975 - accuracy: 0.8176 - val_loss: 1.4554 - val_accuracy: 0.6549\n",
      "Epoch 39/40\n",
      "227/227 [==============================] - 1s 3ms/step l\n",
      "2036/2036 [==============================] - 9s 5ms/step - loss: 0.5874 - accuracy: 0.8202 - val_loss: 1.4694 - val_accuracy: 0.6557\n",
      "Epoch 40/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.5796 - accura\n",
      "2036/2036 [==============================] - 5s 2ms/step - loss: 0.5796 - accuracy: 0.8224 - val_loss: 1.4915 - val_accuracy: 0.6575\n",
      "402/402 [==============================] - 0s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8691    0.3785    0.5273      1070\n",
      "         120     0.2634    0.6531    0.3754       196\n",
      "         125     0.6765    0.8214    0.7419       532\n",
      "         134     0.7857    0.5789    0.6667        19\n",
      "         190     0.5630    0.6700    0.6119       200\n",
      "          20     0.4936    0.2877    0.3635       810\n",
      "         200     0.7359    0.4864    0.5857       590\n",
      "         203     0.4524    0.7037    0.5507        27\n",
      "          22     0.7496    0.8147    0.7808       518\n",
      "         269     0.3646    0.3302    0.3465       106\n",
      "         276     0.1651    0.2812    0.2081        64\n",
      "         287     0.3477    0.6526    0.4537       285\n",
      "         295     0.5684    0.6667    0.6136        81\n",
      "         306     0.3457    0.2979    0.3200        94\n",
      "         312     0.2857    0.2381    0.2597        42\n",
      "         319     0.4600    0.4510    0.4554        51\n",
      "         326     0.3043    0.2258    0.2593        31\n",
      "         327     0.3478    0.2286    0.2759        35\n",
      "         345     0.0833    0.1154    0.0968        26\n",
      "         347     0.2857    0.4167    0.3390        24\n",
      "         352     0.3568    0.8168    0.4966       453\n",
      "         362     0.3918    0.5492    0.4573       122\n",
      "         400     0.3261    0.4348    0.3727       138\n",
      "         401     0.4444    0.4615    0.4528        52\n",
      "         415     0.6585    0.6429    0.6506        42\n",
      "         416     0.4952    0.7969    0.6108       325\n",
      "         426     0.4286    0.5714    0.4898        42\n",
      "         427     0.3537    0.6591    0.4603        44\n",
      "         434     0.4242    0.7784    0.5491       194\n",
      "         476     0.4534    0.7848    0.5747       223\n",
      "         502     0.3941    0.7477    0.5161       107\n",
      "         522     0.3566    0.5667    0.4378        90\n",
      "         532     0.4571    0.7273    0.5614        44\n",
      "          59     0.8182    0.7431    0.7788       109\n",
      "         601     0.3916    0.7887    0.5234        71\n",
      "         611     0.6066    0.8043    0.6916        92\n",
      "         617     0.4783    0.5789    0.5238        38\n",
      "         639     0.2500    0.3214    0.2813        28\n",
      "         668     0.0390    0.0667    0.0492        45\n",
      "         732     0.3582    0.2449    0.2909        98\n",
      "          74     0.1280    0.3803    0.1915        71\n",
      "         755     0.3846    0.3571    0.3704        28\n",
      "          77     0.3147    0.4932    0.3842       148\n",
      "         770     0.2118    0.3051    0.2500        59\n",
      "         772     0.5385    0.4118    0.4667        34\n",
      "          78     0.5254    0.6993    0.6000       296\n",
      "         787     0.6940    0.4944    0.5774       890\n",
      "          79     0.9785    0.4645    0.6300      2256\n",
      "         798     0.7308    0.7364    0.7336       129\n",
      "         835     0.5000    0.7805    0.6095        41\n",
      "         843     0.3333    0.4412    0.3797        34\n",
      "         862     0.5667    0.6182    0.5913       220\n",
      "         863     0.2463    0.2797    0.2619       118\n",
      "          89     0.9885    0.6311    0.7704       957\n",
      "         908     0.7222    0.5417    0.6190        24\n",
      "         918     0.5889    0.5889    0.5889        90\n",
      "          94     0.3280    0.5651    0.4151       292\n",
      "\n",
      "    accuracy                         0.5496     12845\n",
      "   macro avg     0.4633    0.5293    0.4744     12845\n",
      "weighted avg     0.6601    0.5496    0.5616     12845\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_no_subj.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_no_subj.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_description_no_subject_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_description_no_subject_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'CWE_classes.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference CVE description with no subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402/402 [==============================] - 0s 997us/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8691    0.3785    0.5273      1070\n",
      "         120     0.2634    0.6531    0.3754       196\n",
      "         125     0.6765    0.8214    0.7419       532\n",
      "         134     0.7857    0.5789    0.6667        19\n",
      "         190     0.5630    0.6700    0.6119       200\n",
      "          20     0.4936    0.2877    0.3635       810\n",
      "         200     0.7359    0.4864    0.5857       590\n",
      "         203     0.4524    0.7037    0.5507        27\n",
      "          22     0.7496    0.8147    0.7808       518\n",
      "         269     0.3646    0.3302    0.3465       106\n",
      "         276     0.1651    0.2812    0.2081        64\n",
      "         287     0.3477    0.6526    0.4537       285\n",
      "         295     0.5684    0.6667    0.6136        81\n",
      "         306     0.3457    0.2979    0.3200        94\n",
      "         312     0.2857    0.2381    0.2597        42\n",
      "         319     0.4600    0.4510    0.4554        51\n",
      "         326     0.3043    0.2258    0.2593        31\n",
      "         327     0.3478    0.2286    0.2759        35\n",
      "         345     0.0833    0.1154    0.0968        26\n",
      "         347     0.2857    0.4167    0.3390        24\n",
      "         352     0.3568    0.8168    0.4966       453\n",
      "         362     0.3918    0.5492    0.4573       122\n",
      "         400     0.3261    0.4348    0.3727       138\n",
      "         401     0.4444    0.4615    0.4528        52\n",
      "         415     0.6585    0.6429    0.6506        42\n",
      "         416     0.4952    0.7969    0.6108       325\n",
      "         426     0.4286    0.5714    0.4898        42\n",
      "         427     0.3537    0.6591    0.4603        44\n",
      "         434     0.4242    0.7784    0.5491       194\n",
      "         476     0.4534    0.7848    0.5747       223\n",
      "         502     0.3941    0.7477    0.5161       107\n",
      "         522     0.3566    0.5667    0.4378        90\n",
      "         532     0.4571    0.7273    0.5614        44\n",
      "          59     0.8182    0.7431    0.7788       109\n",
      "         601     0.3916    0.7887    0.5234        71\n",
      "         611     0.6066    0.8043    0.6916        92\n",
      "         617     0.4783    0.5789    0.5238        38\n",
      "         639     0.2500    0.3214    0.2813        28\n",
      "         668     0.0390    0.0667    0.0492        45\n",
      "         732     0.3582    0.2449    0.2909        98\n",
      "          74     0.1280    0.3803    0.1915        71\n",
      "         755     0.3846    0.3571    0.3704        28\n",
      "          77     0.3147    0.4932    0.3842       148\n",
      "         770     0.2118    0.3051    0.2500        59\n",
      "         772     0.5385    0.4118    0.4667        34\n",
      "          78     0.5254    0.6993    0.6000       296\n",
      "         787     0.6940    0.4944    0.5774       890\n",
      "          79     0.9785    0.4645    0.6300      2256\n",
      "         798     0.7308    0.7364    0.7336       129\n",
      "         835     0.5000    0.7805    0.6095        41\n",
      "         843     0.3333    0.4412    0.3797        34\n",
      "         862     0.5667    0.6182    0.5913       220\n",
      "         863     0.2463    0.2797    0.2619       118\n",
      "          89     0.9885    0.6311    0.7704       957\n",
      "         908     0.7222    0.5417    0.6190        24\n",
      "         918     0.5889    0.5889    0.5889        90\n",
      "          94     0.3280    0.5651    0.4151       292\n",
      "\n",
      "    accuracy                         0.5496     12845\n",
      "   macro avg     0.4633    0.5293    0.4744     12845\n",
      "weighted avg     0.6601    0.5496    0.5616     12845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "best_model = joblib.load('CWE_classes.joblib')\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder_train = joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Load the test data\n",
    "with open('test_no_subj.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_description_no_subject_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert the predicted labels back to their original form\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
