{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on core terms and consequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "Epoch 1 - F1 Score: 0.6190\n",
      "Saved best model\n",
      "[0.6189759572014787]\n",
      "2036/2036 [==============================] - 15s 7ms/step - loss: 1.7410 - accuracy: 0.5388 - val_loss: 1.2747 - val_accuracy: 0.6543\n",
      "Epoch 2/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 1.1833 \n",
      "Epoch 2 - F1 Score: 0.6712\n",
      "Saved best model\n",
      "[0.6189759572014787, 0.6711952515691483]\n",
      "2036/2036 [==============================] - 11s 6ms/step - loss: 1.1831 - accuracy: 0.6713 - val_loss: 1.1291 - val_accuracy: 0.6888\n",
      "Epoch 3/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 1.089\n",
      "Epoch 3 - F1 Score: 0.6938\n",
      "Saved best model\n",
      "[0.6189759572014787, 0.6711952515691483, 0.693774162906171]\n",
      "2036/2036 [==============================] - 9s 5ms/step - loss: 1.0898 - accuracy: 0.6938 - val_loss: 1.0813 - val_accuracy: 0.7066\n",
      "Epoch 4/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 1.0407 \n",
      "Epoch 4 - F1 Score: 0.7032\n",
      "Saved best model\n",
      "[0.6189759572014787, 0.6711952515691483, 0.693774162906171, 0.7031763275733425]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 1.0404 - accuracy: 0.7054 - val_loss: 1.0439 - val_accuracy: 0.7148\n",
      "Epoch 5/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 1.008\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 1.0085 - accuracy: 0.7123 - val_loss: 1.0341 - val_accuracy: 0.7145\n",
      "Epoch 6/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.983\n",
      "Epoch 6 - F1 Score: 0.7100\n",
      "Saved best model\n",
      "[0.6189759572014787, 0.6711952515691483, 0.693774162906171, 0.7031763275733425, 0.7008649512751347, 0.7099827378030305]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9831 - accuracy: 0.7189 - val_loss: 1.0123 - val_accuracy: 0.7213\n",
      "Epoch 7/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.962\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9621 - accuracy: 0.7245 - val_loss: 1.0130 - val_accuracy: 0.7142\n",
      "Epoch 8/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.9455 \n",
      "Epoch 8 - F1 Score: 0.7188\n",
      "Saved best model\n",
      "[0.6189759572014787, 0.6711952515691483, 0.693774162906171, 0.7031763275733425, 0.7008649512751347, 0.7099827378030305, 0.7076204460086281, 0.7188322535280542]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9454 - accuracy: 0.7281 - val_loss: 0.9917 - val_accuracy: 0.7260\n",
      "Epoch 9/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.9282 \n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9278 - accuracy: 0.7316 - val_loss: 0.9911 - val_accuracy: 0.7195\n",
      "Epoch 10/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.9161 \n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9160 - accuracy: 0.7350 - val_loss: 0.9826 - val_accuracy: 0.7260\n",
      "Epoch 11/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.9001 \n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9011 - accuracy: 0.7370 - val_loss: 0.9853 - val_accuracy: 0.7245\n",
      "Epoch 12/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.8894 \n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.8902 - accuracy: 0.7401 - val_loss: 0.9973 - val_accuracy: 0.7254\n",
      "Epoch 13/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.8796 - \n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.8792 - accuracy: 0.7431 - val_loss: 0.9885 - val_accuracy: 0.7246\n",
      "Epoch 14/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "2036/2036 [==============================] - 10s 5ms/step - loss: 0.8673 - accuracy: 0.7458 - val_loss: 0.9793 - val_accuracy: 0.7261\n",
      "Epoch 15/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "Epoch 15 - F1 Score: 0.7219\n",
      "Saved best model\n",
      "[0.6189759572014787, 0.6711952515691483, 0.693774162906171, 0.7031763275733425, 0.7008649512751347, 0.7099827378030305, 0.7076204460086281, 0.7188322535280542, 0.7124797444933397, 0.7135501923826866, 0.7169083673453605, 0.7177303491805518, 0.7161140064039652, 0.7167632227250957, 0.7218509419376743]\n",
      "2036/2036 [==============================] - 17s 9ms/step - loss: 0.8587 - accuracy: 0.7480 - val_loss: 0.9691 - val_accuracy: 0.7272\n",
      "Epoch 16/40\n",
      "227/227 [==============================] - 1s 3ms/step lo\n",
      "2036/2036 [==============================] - 17s 8ms/step - loss: 0.8468 - accuracy: 0.7500 - val_loss: 0.9753 - val_accuracy: 0.7307\n",
      "Epoch 17/40\n",
      "227/227 [==============================] - 1s 4ms/step \n",
      "2036/2036 [==============================] - 17s 8ms/step - loss: 0.8378 - accuracy: 0.7530 - val_loss: 0.9843 - val_accuracy: 0.7253\n",
      "Epoch 18/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "2036/2036 [==============================] - 17s 9ms/step - loss: 0.8287 - accuracy: 0.7539 - val_loss: 0.9920 - val_accuracy: 0.7252\n",
      "Epoch 19/40\n",
      "227/227 [==============================] - 1s 3ms/step lo\n",
      "2036/2036 [==============================] - 17s 8ms/step - loss: 0.8176 - accuracy: 0.7567 - val_loss: 0.9989 - val_accuracy: 0.7239\n",
      "Epoch 20/40\n",
      "227/227 [==============================] - 1s 4ms/step \n",
      "2036/2036 [==============================] - 17s 8ms/step - loss: 0.8091 - accuracy: 0.7584 - val_loss: 0.9759 - val_accuracy: 0.7265\n",
      "Epoch 21/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "2036/2036 [==============================] - 20s 10ms/step - loss: 0.8014 - accuracy: 0.7609 - val_loss: 0.9822 - val_accuracy: 0.7260\n",
      "Epoch 22/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7946 \n",
      "Epoch 22 - F1 Score: 0.7264\n",
      "Saved best model\n",
      "[0.6189759572014787, 0.6711952515691483, 0.693774162906171, 0.7031763275733425, 0.7008649512751347, 0.7099827378030305, 0.7076204460086281, 0.7188322535280542, 0.7124797444933397, 0.7135501923826866, 0.7169083673453605, 0.7177303491805518, 0.7161140064039652, 0.7167632227250957, 0.7218509419376743, 0.7210332535059365, 0.7180186594918438, 0.7167689993243782, 0.7179139944087921, 0.7204630344104652, 0.7178118702538587, 0.7263679572726842]\n",
      "2036/2036 [==============================] - 15s 7ms/step - loss: 0.7946 - accuracy: 0.7621 - val_loss: 0.9889 - val_accuracy: 0.7271\n",
      "Epoch 23/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7826 - \n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.7823 - accuracy: 0.7662 - val_loss: 0.9808 - val_accuracy: 0.7321\n",
      "Epoch 24/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7764 - \n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 0.7763 - accuracy: 0.7675 - val_loss: 0.9896 - val_accuracy: 0.7288\n",
      "Epoch 25/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7665 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7664 - accuracy: 0.7695 - val_loss: 0.9961 - val_accuracy: 0.7253\n",
      "Epoch 26/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7605 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7606 - accuracy: 0.7703 - val_loss: 1.0028 - val_accuracy: 0.7259\n",
      "Epoch 27/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7503 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7507 - accuracy: 0.7724 - val_loss: 1.0085 - val_accuracy: 0.7238\n",
      "Epoch 28/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7441 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7433 - accuracy: 0.7741 - val_loss: 1.0071 - val_accuracy: 0.7256\n",
      "Epoch 29/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7352 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7352 - accuracy: 0.7777 - val_loss: 1.0102 - val_accuracy: 0.7256\n",
      "Epoch 30/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7286 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7288 - accuracy: 0.7776 - val_loss: 1.0218 - val_accuracy: 0.7242\n",
      "Epoch 31/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7207 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7209 - accuracy: 0.7794 - val_loss: 1.0439 - val_accuracy: 0.7223\n",
      "Epoch 32/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7142 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7142 - accuracy: 0.7822 - val_loss: 1.0178 - val_accuracy: 0.7196\n",
      "Epoch 33/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7077 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7078 - accuracy: 0.7838 - val_loss: 1.0258 - val_accuracy: 0.7223\n",
      "Epoch 34/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7008 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.7006 - accuracy: 0.7863 - val_loss: 1.0407 - val_accuracy: 0.7241\n",
      "Epoch 35/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6931 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6930 - accuracy: 0.7876 - val_loss: 1.0520 - val_accuracy: 0.7231\n",
      "Epoch 36/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6871 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6870 - accuracy: 0.7906 - val_loss: 1.0382 - val_accuracy: 0.7234\n",
      "Epoch 37/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6802 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6801 - accuracy: 0.7904 - val_loss: 1.0606 - val_accuracy: 0.7230\n",
      "Epoch 38/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6712 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6713 - accuracy: 0.7936 - val_loss: 1.0698 - val_accuracy: 0.7220\n",
      "Epoch 39/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.6663 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6668 - accuracy: 0.7940 - val_loss: 1.0474 - val_accuracy: 0.7235\n",
      "Epoch 40/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6614 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6617 - accuracy: 0.7954 - val_loss: 1.0576 - val_accuracy: 0.7210\n",
      "402/402 [==============================] - 1s 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8272    0.4430    0.5770      1070\n",
      "         120     0.3222    0.7398    0.4489       196\n",
      "         125     0.8021    0.8459    0.8234       532\n",
      "         134     0.8421    0.8421    0.8421        19\n",
      "         190     0.7542    0.8900    0.8165       200\n",
      "          20     0.5815    0.1938    0.2907       810\n",
      "         200     0.6534    0.5305    0.5856       590\n",
      "         203     0.5946    0.8148    0.6875        27\n",
      "          22     0.8724    0.8977    0.8849       518\n",
      "         269     0.4466    0.4340    0.4402       106\n",
      "         276     0.3492    0.3438    0.3465        64\n",
      "         287     0.4575    0.7368    0.5645       285\n",
      "         295     0.5495    0.7531    0.6354        81\n",
      "         306     0.4247    0.3298    0.3713        94\n",
      "         312     0.4545    0.2381    0.3125        42\n",
      "         319     0.6444    0.5686    0.6042        51\n",
      "         326     0.4444    0.1290    0.2000        31\n",
      "         327     0.3519    0.5429    0.4270        35\n",
      "         345     0.2778    0.3846    0.3226        26\n",
      "         347     0.4062    0.5417    0.4643        24\n",
      "         352     0.6667    0.9581    0.7862       453\n",
      "         362     0.6783    0.7951    0.7321       122\n",
      "         400     0.3990    0.5725    0.4702       138\n",
      "         401     0.5577    0.5577    0.5577        52\n",
      "         415     0.7917    0.9048    0.8444        42\n",
      "         416     0.8446    0.8862    0.8649       325\n",
      "         426     0.7273    0.5714    0.6400        42\n",
      "         427     0.5522    0.8409    0.6667        44\n",
      "         434     0.5325    0.8866    0.6654       194\n",
      "         476     0.6529    0.8520    0.7393       223\n",
      "         502     0.6466    0.8037    0.7167       107\n",
      "         522     0.4724    0.6667    0.5530        90\n",
      "         532     0.3820    0.7727    0.5113        44\n",
      "          59     0.6746    0.7798    0.7234       109\n",
      "         601     0.5167    0.8732    0.6492        71\n",
      "         611     0.7586    0.9565    0.8462        92\n",
      "         617     0.5200    0.6842    0.5909        38\n",
      "         639     0.6667    0.5714    0.6154        28\n",
      "         668     0.0879    0.1778    0.1176        45\n",
      "         732     0.2687    0.3673    0.3103        98\n",
      "          74     0.1732    0.3099    0.2222        71\n",
      "         755     0.1667    0.5714    0.2581        28\n",
      "          77     0.4903    0.5135    0.5017       148\n",
      "         770     0.2533    0.3220    0.2836        59\n",
      "         772     0.4130    0.5588    0.4750        34\n",
      "          78     0.5767    0.6858    0.6265       296\n",
      "         787     0.7044    0.6775    0.6907       890\n",
      "          79     0.9965    0.7655    0.8659      2256\n",
      "         798     0.8095    0.7907    0.8000       129\n",
      "         835     0.6415    0.8293    0.7234        41\n",
      "         843     0.7297    0.7941    0.7606        34\n",
      "         862     0.6603    0.6273    0.6434       220\n",
      "         863     0.2437    0.4068    0.3048       118\n",
      "          89     0.9919    0.8955    0.9412       957\n",
      "         908     0.4857    0.7083    0.5763        24\n",
      "         918     0.6639    0.8778    0.7560        90\n",
      "          94     0.4334    0.6575    0.5224       292\n",
      "\n",
      "    accuracy                         0.6807     12845\n",
      "   macro avg     0.5594    0.6433    0.5824     12845\n",
      "weighted avg     0.7275    0.6807    0.6827     12845\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_core_cons.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_core_cons.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_core_consequences_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_core_consequences_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference core terms and consequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '<'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load the test data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_core_cons.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[1;32m---> 14\u001b[0m     unbalanced \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcve_core_consequences_ada_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m unbalanced \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcwe\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcwe\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m unbalanced \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcwe\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '<'."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "best_model = joblib.load('best_model.joblib')\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder_train = joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Load the test data\n",
    "with open('test_core_cons.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_core_consequences_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert the predicted labels back to their original form\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '<'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_core_cons.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[1;32m----> 2\u001b[0m     unbalanced \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(unbalanced[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '<'."
     ]
    }
   ],
   "source": [
    "with open('test_core_cons.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "print(unbalanced[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
