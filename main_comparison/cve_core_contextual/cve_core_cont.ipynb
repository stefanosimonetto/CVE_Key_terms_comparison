{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on core terms and contextual terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "Epoch 1 - F1 Score: 0.6236\n",
      "Saved best model\n",
      "[0.6235621435468757]\n",
      "2036/2036 [==============================] - 18s 8ms/step - loss: 1.7705 - accuracy: 0.5343 - val_loss: 1.2652 - val_accuracy: 0.6543\n",
      "Epoch 2/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 1.170\n",
      "Epoch 2 - F1 Score: 0.6720\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 1.1703 - accuracy: 0.6744 - val_loss: 1.1239 - val_accuracy: 0.6878\n",
      "Epoch 3/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 1.061\n",
      "Epoch 3 - F1 Score: 0.6905\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 1.0614 - accuracy: 0.7031 - val_loss: 1.0550 - val_accuracy: 0.7057\n",
      "Epoch 4/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 1.004\n",
      "Epoch 4 - F1 Score: 0.7054\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282, 0.7053533397442603]\n",
      "2036/2036 [==============================] - 9s 5ms/step - loss: 1.0046 - accuracy: 0.7153 - val_loss: 1.0174 - val_accuracy: 0.7135\n",
      "Epoch 5/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.967\n",
      "Epoch 5 - F1 Score: 0.7062\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282, 0.7053533397442603, 0.7062118730991521]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9679 - accuracy: 0.7253 - val_loss: 0.9926 - val_accuracy: 0.7177\n",
      "Epoch 6/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.9399 \n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9396 - accuracy: 0.7330 - val_loss: 1.0234 - val_accuracy: 0.7151\n",
      "Epoch 7/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.915\n",
      "Epoch 7 - F1 Score: 0.7138\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282, 0.7053533397442603, 0.7062118730991521, 0.7048380257406531, 0.7137871537907192]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.9154 - accuracy: 0.7378 - val_loss: 0.9768 - val_accuracy: 0.7237\n",
      "Epoch 8/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.895\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.8948 - accuracy: 0.7431 - val_loss: 0.9825 - val_accuracy: 0.7244\n",
      "Epoch 9/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.8784 \n",
      "Epoch 9 - F1 Score: 0.7184\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282, 0.7053533397442603, 0.7062118730991521, 0.7048380257406531, 0.7137871537907192, 0.7135079741416804, 0.718375916404332]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.8781 - accuracy: 0.7467 - val_loss: 0.9721 - val_accuracy: 0.7241\n",
      "Epoch 10/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.861\n",
      "Epoch 10 - F1 Score: 0.7227\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282, 0.7053533397442603, 0.7062118730991521, 0.7048380257406531, 0.7137871537907192, 0.7135079741416804, 0.718375916404332, 0.7226976157821265]\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.8621 - accuracy: 0.7499 - val_loss: 0.9618 - val_accuracy: 0.7267\n",
      "Epoch 11/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.846\n",
      "2036/2036 [==============================] - 9s 4ms/step - loss: 0.8466 - accuracy: 0.7538 - val_loss: 0.9694 - val_accuracy: 0.7275\n",
      "Epoch 12/40\n",
      "227/227 [==============================] - 1s 2ms/step loss: 0.830\n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.8311 - accuracy: 0.7575 - val_loss: 0.9677 - val_accuracy: 0.7266\n",
      "Epoch 13/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.8189 \n",
      "2036/2036 [==============================] - 8s 4ms/step - loss: 0.8192 - accuracy: 0.7593 - val_loss: 0.9730 - val_accuracy: 0.7249\n",
      "Epoch 14/40\n",
      "227/227 [==============================] - 1s 3ms/step \n",
      "Epoch 14 - F1 Score: 0.7311\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282, 0.7053533397442603, 0.7062118730991521, 0.7048380257406531, 0.7137871537907192, 0.7135079741416804, 0.718375916404332, 0.7226976157821265, 0.7178505666265025, 0.7209660161537194, 0.7147899559345117, 0.7311270222574553]\n",
      "2036/2036 [==============================] - 14s 7ms/step - loss: 0.8075 - accuracy: 0.7625 - val_loss: 0.9564 - val_accuracy: 0.7347\n",
      "Epoch 15/40\n",
      "227/227 [==============================] - 1s 3ms/step lo\n",
      "2036/2036 [==============================] - 18s 9ms/step - loss: 0.7940 - accuracy: 0.7652 - val_loss: 0.9724 - val_accuracy: 0.7306\n",
      "Epoch 16/40\n",
      "227/227 [==============================] - 1s 3ms/step lo\n",
      "2036/2036 [==============================] - 17s 8ms/step - loss: 0.7831 - accuracy: 0.7686 - val_loss: 0.9646 - val_accuracy: 0.7321\n",
      "Epoch 17/40\n",
      "227/227 [==============================] - 1s 3ms/step lo\n",
      "2036/2036 [==============================] - 17s 9ms/step - loss: 0.7724 - accuracy: 0.7708 - val_loss: 0.9549 - val_accuracy: 0.7314\n",
      "Epoch 18/40\n",
      "227/227 [==============================] - 1s 4ms/step \n",
      "2036/2036 [==============================] - 17s 8ms/step - loss: 0.7595 - accuracy: 0.7754 - val_loss: 0.9833 - val_accuracy: 0.7274\n",
      "Epoch 19/40\n",
      "227/227 [==============================] - 1s 3ms/step lo\n",
      "2036/2036 [==============================] - 17s 8ms/step - loss: 0.7495 - accuracy: 0.7761 - val_loss: 0.9526 - val_accuracy: 0.7317\n",
      "Epoch 20/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "2036/2036 [==============================] - 18s 9ms/step - loss: 0.7383 - accuracy: 0.7786 - val_loss: 0.9578 - val_accuracy: 0.7322\n",
      "Epoch 21/40\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "2036/2036 [==============================] - 20s 10ms/step - loss: 0.7302 - accuracy: 0.7805 - val_loss: 0.9686 - val_accuracy: 0.7364\n",
      "Epoch 22/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7198 - \n",
      "2036/2036 [==============================] - 10s 5ms/step - loss: 0.7200 - accuracy: 0.7832 - val_loss: 0.9712 - val_accuracy: 0.7311\n",
      "Epoch 23/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.7087 - \n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 0.7087 - accuracy: 0.7867 - val_loss: 0.9842 - val_accuracy: 0.7310\n",
      "Epoch 24/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6975 \n",
      "2036/2036 [==============================] - 7s 4ms/step - loss: 0.6976 - accuracy: 0.7913 - val_loss: 0.9831 - val_accuracy: 0.7287\n",
      "Epoch 25/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6892 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6892 - accuracy: 0.7910 - val_loss: 1.0138 - val_accuracy: 0.7223\n",
      "Epoch 26/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6810 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6810 - accuracy: 0.7945 - val_loss: 0.9826 - val_accuracy: 0.7357\n",
      "Epoch 27/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6715 - \n",
      "Epoch 27 - F1 Score: 0.7316\n",
      "Saved best model\n",
      "[0.6235621435468757, 0.6720225985530234, 0.6905123735486282, 0.7053533397442603, 0.7062118730991521, 0.7048380257406531, 0.7137871537907192, 0.7135079741416804, 0.718375916404332, 0.7226976157821265, 0.7178505666265025, 0.7209660161537194, 0.7147899559345117, 0.7311270222574553, 0.7236101179619101, 0.7269938107946583, 0.7256278567552303, 0.7206910838582578, 0.7242584258313252, 0.7264005665863507, 0.7309675354215845, 0.723716429183236, 0.7250022332950844, 0.7205224122450531, 0.7196985561088171, 0.7288179196018121, 0.7315787352057338]\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6716 - accuracy: 0.7965 - val_loss: 0.9935 - val_accuracy: 0.7343\n",
      "Epoch 28/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6624 - \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6629 - accuracy: 0.7987 - val_loss: 0.9912 - val_accuracy: 0.7368\n",
      "Epoch 29/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6529 \n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6529 - accuracy: 0.8009 - val_loss: 1.0073 - val_accuracy: 0.7329\n",
      "Epoch 30/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6462 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6462 - accuracy: 0.8033 - val_loss: 1.0356 - val_accuracy: 0.7305\n",
      "Epoch 31/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6368 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6368 - accuracy: 0.8038 - val_loss: 1.0052 - val_accuracy: 0.7310\n",
      "Epoch 32/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.6290 - ac\n",
      "2036/2036 [==============================] - 6s 3ms/step - loss: 0.6289 - accuracy: 0.8070 - val_loss: 1.0243 - val_accuracy: 0.7262\n",
      "Epoch 33/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6208 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6207 - accuracy: 0.8083 - val_loss: 1.0234 - val_accuracy: 0.7343\n",
      "Epoch 34/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6123 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6124 - accuracy: 0.8125 - val_loss: 1.0347 - val_accuracy: 0.7324\n",
      "Epoch 35/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.6053 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.6053 - accuracy: 0.8123 - val_loss: 1.0295 - val_accuracy: 0.7329\n",
      "Epoch 36/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.5966 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.5966 - accuracy: 0.8157 - val_loss: 1.0527 - val_accuracy: 0.7278\n",
      "Epoch 37/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.5880 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.5883 - accuracy: 0.8171 - val_loss: 1.0832 - val_accuracy: 0.7213\n",
      "Epoch 38/40\n",
      "227/227 [==============================] - 0s 1ms/step loss: 0.5816 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.5818 - accuracy: 0.8195 - val_loss: 1.0500 - val_accuracy: 0.7284\n",
      "Epoch 39/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.5733 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.5731 - accuracy: 0.8215 - val_loss: 1.0696 - val_accuracy: 0.7235\n",
      "Epoch 40/40\n",
      "227/227 [==============================] - 0s 2ms/step loss: 0.5650 - ac\n",
      "2036/2036 [==============================] - 7s 3ms/step - loss: 0.5651 - accuracy: 0.8244 - val_loss: 1.0720 - val_accuracy: 0.7263\n",
      "402/402 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8386    0.4953    0.6228      1070\n",
      "         120     0.4061    0.8163    0.5424       196\n",
      "         125     0.7640    0.8214    0.7917       532\n",
      "         134     0.6923    0.9474    0.8000        19\n",
      "         190     0.6832    0.8950    0.7749       200\n",
      "          20     0.5804    0.3210    0.4134       810\n",
      "         200     0.6912    0.4932    0.5757       590\n",
      "         203     0.2899    0.7407    0.4167        27\n",
      "          22     0.8827    0.8861    0.8844       518\n",
      "         269     0.4268    0.3302    0.3723       106\n",
      "         276     0.3333    0.2812    0.3051        64\n",
      "         287     0.5263    0.6316    0.5742       285\n",
      "         295     0.5631    0.7160    0.6304        81\n",
      "         306     0.3761    0.4362    0.4039        94\n",
      "         312     0.3077    0.2857    0.2963        42\n",
      "         319     0.5686    0.5686    0.5686        51\n",
      "         326     0.3913    0.2903    0.3333        31\n",
      "         327     0.3636    0.4571    0.4051        35\n",
      "         345     0.2500    0.3846    0.3030        26\n",
      "         347     0.5833    0.5833    0.5833        24\n",
      "         352     0.7836    0.9514    0.8594       453\n",
      "         362     0.8053    0.7459    0.7745       122\n",
      "         400     0.3288    0.5290    0.4056       138\n",
      "         401     0.5556    0.4808    0.5155        52\n",
      "         415     0.9286    0.9286    0.9286        42\n",
      "         416     0.8488    0.8985    0.8729       325\n",
      "         426     0.6744    0.6905    0.6824        42\n",
      "         427     0.6957    0.7273    0.7111        44\n",
      "         434     0.6183    0.8351    0.7105       194\n",
      "         476     0.6213    0.8386    0.7137       223\n",
      "         502     0.7190    0.8131    0.7632       107\n",
      "         522     0.3110    0.7222    0.4348        90\n",
      "         532     0.4848    0.7273    0.5818        44\n",
      "          59     0.7611    0.7890    0.7748       109\n",
      "         601     0.6064    0.8028    0.6909        71\n",
      "         611     0.7963    0.9348    0.8600        92\n",
      "         617     0.6111    0.5789    0.5946        38\n",
      "         639     0.4286    0.5357    0.4762        28\n",
      "         668     0.0986    0.1556    0.1207        45\n",
      "         732     0.2217    0.4796    0.3032        98\n",
      "          74     0.1068    0.4648    0.1737        71\n",
      "         755     0.2353    0.5714    0.3333        28\n",
      "          77     0.5372    0.4392    0.4833       148\n",
      "         770     0.3182    0.2373    0.2718        59\n",
      "         772     0.4773    0.6176    0.5385        34\n",
      "          78     0.5679    0.7770    0.6562       296\n",
      "         787     0.7292    0.6685    0.6975       890\n",
      "          79     0.9931    0.7602    0.8612      2256\n",
      "         798     0.8015    0.8140    0.8077       129\n",
      "         835     0.5517    0.7805    0.6465        41\n",
      "         843     0.6279    0.7941    0.7013        34\n",
      "         862     0.5992    0.7000    0.6457       220\n",
      "         863     0.3558    0.3136    0.3333       118\n",
      "          89     0.9915    0.8485    0.9144       957\n",
      "         908     0.2135    0.7917    0.3363        24\n",
      "         918     0.7857    0.8556    0.8191        90\n",
      "          94     0.4410    0.6404    0.5223       292\n",
      "\n",
      "    accuracy                         0.6835     12845\n",
      "   macro avg     0.5570    0.6425    0.5809     12845\n",
      "weighted avg     0.7371    0.6835    0.6940     12845\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder_train.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_model = None\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_val_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "        f1 = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "        \n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.best_model = self.model\n",
    "            print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}\")\n",
    "            print(\"Saved best model\")\n",
    "            print(self.f1_scores)\n",
    "\n",
    "with open('train_core_cont.pickle', 'rb') as f1:\n",
    "    balanced = pickle.load(f1)\n",
    "\n",
    "with open('test_core_cont.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "train = np.array([item['cve_core_contextual_ada_embedding'] for item in balanced if item['cwe'] != 'None'])\n",
    "test = np.array([item['cwe'] for item in balanced if item['cwe'] != 'None'])\n",
    "np.random.seed(42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train,test,test_size=0.1,random_state=42)\n",
    "\n",
    "X_test = np.array([item['cve_core_contextual_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "label_encoder_train = LabelEncoder()\n",
    "y_train_encoded = label_encoder_train.fit_transform(y_train)\n",
    "label_encoder_test = LabelEncoder()\n",
    "y_test_encoded = label_encoder_test.fit_transform(y_test)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "f1_callback = F1ScoreCallback(X_val, label_encoder_train.transform(y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=32, validation_data=(X_val, label_encoder_train.transform(y_val)), verbose=1, callbacks=[f1_callback])\n",
    "\n",
    "best_model = f1_callback.best_model\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model.joblib')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n",
    "\n",
    "joblib.dump(label_encoder_train, 'label_encoder_train.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference core and contextual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402/402 [==============================] - 1s 1ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         119     0.8386    0.4953    0.6228      1070\n",
      "         120     0.4061    0.8163    0.5424       196\n",
      "         125     0.7640    0.8214    0.7917       532\n",
      "         134     0.6923    0.9474    0.8000        19\n",
      "         190     0.6832    0.8950    0.7749       200\n",
      "          20     0.5804    0.3210    0.4134       810\n",
      "         200     0.6912    0.4932    0.5757       590\n",
      "         203     0.2899    0.7407    0.4167        27\n",
      "          22     0.8827    0.8861    0.8844       518\n",
      "         269     0.4268    0.3302    0.3723       106\n",
      "         276     0.3333    0.2812    0.3051        64\n",
      "         287     0.5263    0.6316    0.5742       285\n",
      "         295     0.5631    0.7160    0.6304        81\n",
      "         306     0.3761    0.4362    0.4039        94\n",
      "         312     0.3077    0.2857    0.2963        42\n",
      "         319     0.5686    0.5686    0.5686        51\n",
      "         326     0.3913    0.2903    0.3333        31\n",
      "         327     0.3636    0.4571    0.4051        35\n",
      "         345     0.2500    0.3846    0.3030        26\n",
      "         347     0.5833    0.5833    0.5833        24\n",
      "         352     0.7836    0.9514    0.8594       453\n",
      "         362     0.8053    0.7459    0.7745       122\n",
      "         400     0.3288    0.5290    0.4056       138\n",
      "         401     0.5556    0.4808    0.5155        52\n",
      "         415     0.9286    0.9286    0.9286        42\n",
      "         416     0.8488    0.8985    0.8729       325\n",
      "         426     0.6744    0.6905    0.6824        42\n",
      "         427     0.6957    0.7273    0.7111        44\n",
      "         434     0.6183    0.8351    0.7105       194\n",
      "         476     0.6213    0.8386    0.7137       223\n",
      "         502     0.7190    0.8131    0.7632       107\n",
      "         522     0.3110    0.7222    0.4348        90\n",
      "         532     0.4848    0.7273    0.5818        44\n",
      "          59     0.7611    0.7890    0.7748       109\n",
      "         601     0.6064    0.8028    0.6909        71\n",
      "         611     0.7963    0.9348    0.8600        92\n",
      "         617     0.6111    0.5789    0.5946        38\n",
      "         639     0.4286    0.5357    0.4762        28\n",
      "         668     0.0986    0.1556    0.1207        45\n",
      "         732     0.2217    0.4796    0.3032        98\n",
      "          74     0.1068    0.4648    0.1737        71\n",
      "         755     0.2353    0.5714    0.3333        28\n",
      "          77     0.5372    0.4392    0.4833       148\n",
      "         770     0.3182    0.2373    0.2718        59\n",
      "         772     0.4773    0.6176    0.5385        34\n",
      "          78     0.5679    0.7770    0.6562       296\n",
      "         787     0.7292    0.6685    0.6975       890\n",
      "          79     0.9931    0.7602    0.8612      2256\n",
      "         798     0.8015    0.8140    0.8077       129\n",
      "         835     0.5517    0.7805    0.6465        41\n",
      "         843     0.6279    0.7941    0.7013        34\n",
      "         862     0.5992    0.7000    0.6457       220\n",
      "         863     0.3558    0.3136    0.3333       118\n",
      "          89     0.9915    0.8485    0.9144       957\n",
      "         908     0.2135    0.7917    0.3363        24\n",
      "         918     0.7857    0.8556    0.8191        90\n",
      "          94     0.4410    0.6404    0.5223       292\n",
      "\n",
      "    accuracy                         0.6835     12845\n",
      "   macro avg     0.5570    0.6425    0.5809     12845\n",
      "weighted avg     0.7371    0.6835    0.6940     12845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "best_model = joblib.load('best_model.joblib')\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder_train = joblib.load('label_encoder_train.joblib')\n",
    "\n",
    "# Load the test data\n",
    "with open('test_core_cont.pickle', 'rb') as f2:\n",
    "    unbalanced = pickle.load(f2)\n",
    "\n",
    "X_test = np.array([item['cve_core_contextual_ada_embedding'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "y_test = np.array([item['cwe'] for item in unbalanced if item['cwe'] != 'None'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert the predicted labels back to their original form\n",
    "y_pred_original = label_encoder_train.inverse_transform(y_pred)\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_original, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
